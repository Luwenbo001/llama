{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os.path as osp\n",
    "import transformers\n",
    "from typing import Union, List\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 2\n",
      "GPU 0: NVIDIA A40\n",
      "GPU 1: NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(\"Number of GPUs:\", device_count)\n",
    "    \n",
    "    for i in range(device_count):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7711ab02478c4f2bbb5a211f22669da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = \"../pretrain/nlp/llama/llama-7b/\"\n",
    "device_map = \"auto\"\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# 打印模型的结构\n",
    "# for name, module in model.named_modules():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-1acfae2c9ea1e8fc\n",
      "Reusing dataset json (/home/1004chr/.cache/huggingface/datasets/json/default-1acfae2c9ea1e8fc/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e045a894d3f48a698e9bd104f5c1d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['index', 'instruction', 'input', 'output'],\n",
      "        num_rows: 2490\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['index', 'instruction', 'input', 'output'],\n",
      "        num_rows: 277\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['index', 'instruction', 'input', 'output'],\n",
      "    num_rows: 2490\n",
      "})\n",
      "{'index': '0', 'instruction': 'No Weapons of Mass Destruction Found in Iraq Yet.', 'input': 'Weapons of Mass Destruction Found in Iraq.', 'output': '0'}\n"
     ]
    }
   ],
   "source": [
    "train_data_path = './RTE/train.json'\n",
    "test_data_path = './RTE/dev.json'\n",
    "\n",
    "data_files = {\n",
    "    'train': train_data_path,\n",
    "    'test': test_data_path\n",
    "}\n",
    "data = load_dataset('json', data_files=data_files)\n",
    "\n",
    "print(data)\n",
    "print(data['train'])\n",
    "print(data['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Premise:\n",
      "No Weapons of Mass Destruction Found in Iraq Yet.\n",
      "\n",
      "### Hypothesis:\n",
      "Weapons of Mass Destruction Found in Iraq.\n",
      "\n",
      "Does the hypothesis entail the premise?\n",
      "\n",
      "### Response:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 加载模板文件\n",
    "path_to_template_file = './RTE/template.json'\n",
    "with open(path_to_template_file, 'r') as file:\n",
    "    template = json.load(file)\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    instruction = data_point['instruction']\n",
    "    input_text = data_point.get('input', None)\n",
    "    label = data_point.get('output', None)\n",
    "\n",
    "    if input_text:\n",
    "        res = template[\"prompt_input\"].format(\n",
    "            instruction=instruction, input=input_text\n",
    "        )\n",
    "    else:\n",
    "        res = template[\"prompt_no_input\"].format(\n",
    "            instruction=instruction\n",
    "        )\n",
    "    if label:\n",
    "        res = f\"{res}{label}\"\n",
    "    \n",
    "    return res\n",
    "\n",
    "prompt = generate_prompt(data['train'][0])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "{'input_ids': [0, 835, 6097, 895, 29901, 13, 3782, 1334, 481, 787, 310, 7360, 15435, 4080, 7460, 297, 21375, 29939, 15175, 29889, 13, 13, 2277, 29937, 28984, 720, 6656, 29901, 13, 4806, 481, 787, 310, 7360, 15435, 4080, 7460, 297, 21375, 29939, 29889, 13, 13, 25125, 278, 20051, 875, 737, 278, 5188, 895, 29973, 13, 13, 2277, 29937, 13291, 29901, 13, 29900, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 835, 6097, 895, 29901, 13, 3782, 1334, 481, 787, 310, 7360, 15435, 4080, 7460, 297, 21375, 29939, 15175, 29889, 13, 13, 2277, 29937, 28984, 720, 6656, 29901, 13, 4806, 481, 787, 310, 7360, 15435, 4080, 7460, 297, 21375, 29939, 29889, 13, 13, 25125, 278, 20051, 875, 737, 278, 5188, 895, 29973, 13, 13, 2277, 29937, 13291, 29901, 13, 29900, 0]}\n"
     ]
    }
   ],
   "source": [
    "CUTOFF_LEN = 256\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=True,  \n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < CUTOFF_LEN\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "tokenize_prompt = tokenize(prompt)\n",
    "print(tokenize_prompt.keys())\n",
    "print(tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "{'input_ids': [0, 835, 6097, 895, 29901, 13, 3782, 1334, 481, 787, 310, 7360, 15435, 4080, 7460, 297, 21375, 29939, 15175, 29889, 13, 13, 2277, 29937, 28984, 720, 6656, 29901, 13, 4806, 481, 787, 310, 7360, 15435, 4080, 7460, 297, 21375, 29939, 29889, 13, 13, 25125, 278, 20051, 875, 737, 278, 5188, 895, 29973, 13, 13, 2277, 29937, 13291, 29901, 13, 29900, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 835, 6097, 895, 29901, 13, 3782, 1334, 481, 787, 310, 7360, 15435, 4080, 7460, 297, 21375, 29939, 15175, 29889, 13, 13, 2277, 29937, 28984, 720, 6656, 29901, 13, 4806, 481, 787, 310, 7360, 15435, 4080, 7460, 297, 21375, 29939, 29889, 13, 13, 25125, 278, 20051, 875, 737, 278, 5188, 895, 29973, 13, 13, 2277, 29937, 13291, 29901, 13, 29900, 0]}\n"
     ]
    }
   ],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "tokenize_data = generate_and_tokenize_prompt(data['train'][0])\n",
    "print(tokenize_data.keys())\n",
    "print(tokenize_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/1004chr/.cache/huggingface/datasets/json/default-1acfae2c9ea1e8fc/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde/cache-b8befd04bd68df81.arrow and /home/1004chr/.cache/huggingface/datasets/json/default-1acfae2c9ea1e8fc/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde/cache-ec25260d6346e191.arrow\n",
      "Parameter 'function'=<function generate_and_tokenize_prompt at 0x7bee0026b130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6e9a8c27114a9fa1e29fb2fe581808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2290 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4fe89377e64537a2294e5059dd833b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First data point in train_data: {'index': '776', 'instruction': '20th Century spokesman Rick Dinon said that between the time it applied for the rate increase and changed its mind, the estimated quake losses had ballooned by several hundred million dollars.', 'input': 'Rick Dinon is the senior vice president of 20th Century Insurance Co.', 'output': '0', 'input_ids': [0, 835, 6097, 895, 29901, 13, 29906, 29900, 386, 24027, 805, 23195, 1171, 24218, 15651, 265, 1497, 393, 1546, 278, 931, 372, 7436, 363, 278, 6554, 7910, 322, 3939, 967, 3458, 29892, 278, 15899, 439, 1296, 28495, 750, 6411, 417, 22367, 491, 3196, 6893, 7284, 17208, 29889, 13, 13, 2277, 29937, 28984, 720, 6656, 29901, 13, 29934, 860, 15651, 265, 338, 278, 16336, 11289, 6673, 310, 29871, 29906, 29900, 386, 24027, 512, 7610, 749, 3189, 29889, 13, 13, 25125, 278, 20051, 875, 737, 278, 5188, 895, 29973, 13, 13, 2277, 29937, 13291, 29901, 13, 29900, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 835, 6097, 895, 29901, 13, 29906, 29900, 386, 24027, 805, 23195, 1171, 24218, 15651, 265, 1497, 393, 1546, 278, 931, 372, 7436, 363, 278, 6554, 7910, 322, 3939, 967, 3458, 29892, 278, 15899, 439, 1296, 28495, 750, 6411, 417, 22367, 491, 3196, 6893, 7284, 17208, 29889, 13, 13, 2277, 29937, 28984, 720, 6656, 29901, 13, 29934, 860, 15651, 265, 338, 278, 16336, 11289, 6673, 310, 29871, 29906, 29900, 386, 24027, 512, 7610, 749, 3189, 29889, 13, 13, 25125, 278, 20051, 875, 737, 278, 5188, 895, 29973, 13, 13, 2277, 29937, 13291, 29901, 13, 29900, 0]}\n",
      "First data point in val_data: {'index': '1913', 'instruction': 'Thanks to a global ban on the ivory trade that was passed in 1989 by the Convention on International Trade in Endangered Species of Wild Fauna and Flora (CITES), the African elephant population may be reversing its spiral toward extinction', 'input': 'The ban on ivory trade has been effective in protecting the elephant from extinction.', 'output': '1', 'input_ids': [0, 835, 6097, 895, 29901, 13, 16894, 304, 263, 5534, 9892, 373, 278, 20444, 706, 11302, 393, 471, 4502, 297, 29871, 29896, 29929, 29947, 29929, 491, 278, 26774, 373, 4623, 27226, 297, 2796, 4600, 287, 21807, 310, 11821, 7748, 4347, 322, 23267, 313, 29907, 1806, 2890, 511, 278, 11715, 4552, 561, 424, 4665, 1122, 367, 18764, 292, 967, 6337, 284, 11183, 1294, 16807, 13, 13, 2277, 29937, 28984, 720, 6656, 29901, 13, 1576, 9892, 373, 20444, 706, 11302, 756, 1063, 11828, 297, 12566, 292, 278, 4552, 561, 424, 515, 1294, 16807, 29889, 13, 13, 25125, 278, 20051, 875, 737, 278, 5188, 895, 29973, 13, 13, 2277, 29937, 13291, 29901, 13, 29896, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 835, 6097, 895, 29901, 13, 16894, 304, 263, 5534, 9892, 373, 278, 20444, 706, 11302, 393, 471, 4502, 297, 29871, 29896, 29929, 29947, 29929, 491, 278, 26774, 373, 4623, 27226, 297, 2796, 4600, 287, 21807, 310, 11821, 7748, 4347, 322, 23267, 313, 29907, 1806, 2890, 511, 278, 11715, 4552, 561, 424, 4665, 1122, 367, 18764, 292, 967, 6337, 284, 11183, 1294, 16807, 13, 13, 2277, 29937, 28984, 720, 6656, 29901, 13, 1576, 9892, 373, 20444, 706, 11302, 756, 1063, 11828, 297, 12566, 292, 278, 4552, 561, 424, 515, 1294, 16807, 29889, 13, 13, 25125, 278, 20051, 875, 737, 278, 5188, 895, 29973, 13, 13, 2277, 29937, 13291, 29901, 13, 29896, 0]}\n"
     ]
    }
   ],
   "source": [
    "train_val = data[\"train\"].train_test_split(\n",
    "    test_size=200, shuffle=True, seed=42\n",
    ")\n",
    "train_data = (\n",
    "    train_val[\"train\"].map(generate_and_tokenize_prompt)\n",
    ")\n",
    "val_data = (\n",
    "    train_val[\"test\"].map(generate_and_tokenize_prompt)\n",
    ")\n",
    "\n",
    "# 输出第一条数据\n",
    "print(\"First data point in train_data:\", train_data[0])\n",
    "print(\"First data point in val_data:\", val_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Premise:\n",
      "No Weapons of Mass Destruction Found in Iraq Yet.\n",
      "\n",
      "### Hypothesis:\n",
      "Weapons of Mass Destruction Found in Iraq.\n",
      "\n",
      "Does the hypothesis entail the premise?\n",
      "\n",
      "### Response:\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/1004chr/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/1004chr/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/1004chr/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:427: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/1004chr/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/1004chr/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/1004chr/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:427: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "/home/1004chr/miniconda3/envs/py310/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>### Premise:\n",
      "No Weapons of Mass Destruction Found in Iraq Yet.\n",
      "\n",
      "### Hypothesis:\n",
      "Weapons of Mass Destruction Found in Iraq.\n",
      "\n",
      "Does the hypothesis entail the premise?\n",
      "\n",
      "### Response:\n",
      "0\n",
      "\n",
      "### Hypothesis:\n",
      "Weapons of Mass Destruction Found in Iraq.\n",
      "\n",
      "Does the hypothesis entail the premise?\n",
      "\n",
      "### Response:\n",
      "0\n",
      "\n",
      "### Hypothesis:\n",
      "Weapons of Mass Destruction Found in Iraq.\n",
      "\n",
      "Does the hypothesis entail the premise?\n",
      "\n",
      "### Response:\n",
      "0\n",
      "\n",
      "### Hypothesis:\n",
      "Weapons of Mass Destruction Found in Iraq.\n",
      "\n",
      "Does the hypothesis entail the premise?\n",
      "\n",
      "### Response:\n",
      "0\n",
      "\n",
      "### Hypothesis:\n"
     ]
    }
   ],
   "source": [
    "# test model infer.\n",
    "model.eval()\n",
    "infer_tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "data_point = data['train'][0]\n",
    "prompt = generate_prompt(data_point)\n",
    "print(prompt)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "# print(inputs)\n",
    "# print(input_ids)\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "s = generation_output.sequences[0]\n",
    "output = tokenizer.decode(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.split(template[\"response_split\"])[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT= 0.05\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "MICRO_BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "LEARNING_RATE = 3e-4\n",
    "TRAIN_STEPS = 300\n",
    "OUTPUT_DIR = \"experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "model.print_trainable_parameters()  # Be more transparent about the % of trainable params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = transformers.TrainingArguments(\n",
    "     per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "     gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "     warmup_steps=100,\n",
    "     max_steps=TRAIN_STEPS,\n",
    "     learning_rate=LEARNING_RATE,\n",
    "     fp16=True,\n",
    "     logging_steps=10,\n",
    "     optim=\"adamw_torch\",\n",
    "     evaluation_strategy=\"steps\",\n",
    "     save_strategy=\"steps\",\n",
    "     eval_steps=50,\n",
    "     save_steps=50,\n",
    "     output_dir=OUTPUT_DIR,\n",
    "     save_total_limit=3,\n",
    "     load_best_model_at_end=True,\n",
    "     report_to=\"none\"\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collated input_ids: tensor([[    0,     0,     0,  ..., 29901,    13,     0],\n",
      "        [    0,     0,     0,  ..., 29901,    13,     0],\n",
      "        [    0,     0,     0,  ..., 29901,    13,     0],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ..., 29901,    13,     0],\n",
      "        [    0,     0,     0,  ..., 29901,    13,     0],\n",
      "        [    0,     0,     0,  ..., 29901,    13,     0]])\n",
      "Collated attention_mask: tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]])\n",
      "Collated labels: tensor([[ -100,  -100,  -100,  ..., 29901,    13,     0],\n",
      "        [ -100,  -100,  -100,  ..., 29901,    13,     0],\n",
      "        [ -100,  -100,  -100,  ..., 29901,    13,     0],\n",
      "        ...,\n",
      "        [ -100,  -100,  -100,  ..., 29901,    13,     0],\n",
      "        [ -100,  -100,  -100,  ..., 29901,    13,     0],\n",
      "        [ -100,  -100,  -100,  ..., 29901,    13,     0]])\n"
     ]
    }
   ],
   "source": [
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    ")\n",
    "\n",
    "# 映射生成和标记化函数\n",
    "tokenized_data = [generate_and_tokenize_prompt(data['train'][i]) for i in list(range(10))]\n",
    "collated_batch = data_collator(tokenized_data)\n",
    "\n",
    "# 输出处理后的数据\n",
    "print(\"Collated input_ids:\", collated_batch['input_ids'])\n",
    "print(\"Collated attention_mask:\", collated_batch['attention_mask'])\n",
    "print(\"Collated labels:\", collated_batch['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/1004chr/.local/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "model.config.use_cache = False\n",
    "old_state_dict = model.state_dict\n",
    "\n",
    "# These lines need to be commented out, torch and peft libraries are incompatible\n",
    "# model.state_dict = (\n",
    "#     lambda self, *_, **__: get_peft_model_state_dict(\n",
    "#         self, old_state_dict()\n",
    "#     )\n",
    "# ).__get__(model, type(model))\n",
    "\n",
    "# model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/1004chr/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/1004chr/miniconda3/envs/py310/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='301' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 4:21:35, Epoch 16.75/18]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.572500</td>\n",
       "      <td>1.537473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.411600</td>\n",
       "      <td>1.445972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.361300</td>\n",
       "      <td>1.433290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.324700</td>\n",
       "      <td>1.431099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.287800</td>\n",
       "      <td>1.434796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.274100</td>\n",
       "      <td>1.437509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/1004chr/miniconda3/envs/py310/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ../pretrain/nlp/llama/llama-7b/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/1004chr/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/1004chr/miniconda3/envs/py310/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/1004chr/miniconda3/envs/py310/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ../pretrain/nlp/llama/llama-7b/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/1004chr/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/1004chr/miniconda3/envs/py310/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/1004chr/miniconda3/envs/py310/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ../pretrain/nlp/llama/llama-7b/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/1004chr/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/1004chr/miniconda3/envs/py310/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/1004chr/miniconda3/envs/py310/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ../pretrain/nlp/llama/llama-7b/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/1004chr/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/1004chr/miniconda3/envs/py310/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/1004chr/miniconda3/envs/py310/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ../pretrain/nlp/llama/llama-7b/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/1004chr/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/1004chr/miniconda3/envs/py310/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/1004chr/miniconda3/envs/py310/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ../pretrain/nlp/llama/llama-7b/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "SafetensorError",
     "evalue": "Error while deserializing header: InvalidHeaderDeserialization",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSafetensorError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2077\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2075\u001b[0m         smp\u001b[38;5;241m.\u001b[39mbarrier()\n\u001b[0;32m-> 2077\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_best_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[38;5;66;03m# add remaining tr_loss\u001b[39;00m\n\u001b[1;32m   2080\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_loss_scalar \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2329\u001b[0m, in \u001b[0;36mTrainer._load_best_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactive_adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(best_adapter_model_path) \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(best_safe_adapter_model_path):\n\u001b[0;32m-> 2329\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_model_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_adapter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2330\u001b[0m         \u001b[38;5;66;03m# Load_adapter has no return value present, modify it when appropriate.\u001b[39;00m\n\u001b[1;32m   2331\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IncompatibleKeys\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/peft/peft_model.py:984\u001b[0m, in \u001b[0;36mPeftModel.load_adapter\u001b[0;34m(self, model_id, adapter_name, is_trainable, torch_device, **kwargs)\u001b[0m\n\u001b[1;32m    981\u001b[0m         peft_config\u001b[38;5;241m.\u001b[39minference_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m is_trainable\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_adapter(adapter_name, peft_config)\n\u001b[0;32m--> 984\u001b[0m adapters_weights \u001b[38;5;241m=\u001b[39m \u001b[43mload_peft_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhf_hub_download_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# load the weights into the model\u001b[39;00m\n\u001b[1;32m    987\u001b[0m ignore_mismatched_sizes \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore_mismatched_sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/peft/utils/save_and_load.py:444\u001b[0m, in \u001b[0;36mload_peft_weights\u001b[0;34m(model_id, device, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m         adapters_weights \u001b[38;5;241m=\u001b[39m safe_load_file(filename, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         adapters_weights \u001b[38;5;241m=\u001b[39m \u001b[43msafe_load_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m     adapters_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(filename, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(device))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/safetensors/torch.py:308\u001b[0m, in \u001b[0;36mload_file\u001b[0;34m(filename, device)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03mLoads a safetensors file into torch format.\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    307\u001b[0m result \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msafe_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    310\u001b[0m         result[k] \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mget_tensor(k)\n",
      "\u001b[0;31mSafetensorError\u001b[0m: Error while deserializing header: InvalidHeaderDeserialization"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
